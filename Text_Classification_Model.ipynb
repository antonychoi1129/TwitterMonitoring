{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NotCleanedModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XwK2Apr_vLG",
        "outputId": "e2068735-2acc-4275-ada9-80c70fbecce4"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# load all docs in a directory\n",
        "def data_pre_process(filename, labels, class_index):\n",
        "    documents = []\n",
        "    file = open(filename, 'r')\n",
        "    # Using readlines() \n",
        "    Lines = file.readlines() \n",
        "    file.close()\n",
        "    # Strips the newline character \n",
        "    for line in Lines: \n",
        "        # print(line)\n",
        "        documents.append(line.replace(\"\\n\", \"\"))\n",
        "        labels.append(class_index)\n",
        "    print(len(documents))\n",
        "    return documents\n",
        "\n",
        "samples = []\n",
        "labels = []\n",
        "class_names = ['negative','neutral','positive']\n",
        "samples.extend(data_pre_process('/content/drive/MyDrive/labelled_data/neg.txt', labels, 0))\n",
        "samples.extend(data_pre_process('/content/drive/MyDrive/labelled_data/neu.txt', labels, 1))\n",
        "samples.extend(data_pre_process('/content/drive/MyDrive/labelled_data/pos.txt', labels, 2))\n",
        "\n",
        "# samples.extend(data_pre_process('/content/drive/MyDrive/labelled_data/test_data/neg.txt', labels, 0))\n",
        "# samples.extend(data_pre_process('/content/drive/MyDrive/labelled_data/test_data/neu.txt', labels, 1))\n",
        "# samples.extend(data_pre_process('/content/drive/MyDrive/labelled_data/test_data/pos.txt', labels, 2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))\n",
        "# print(samples)\n",
        "# print(labels[:20])\n",
        "# print(labels[20:])\n",
        "\n",
        "# Shuffle the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "vectorizer.get_vocabulary()[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "100000\n",
            "100000\n",
            "Classes: ['negative', 'neutral', 'positive']\n",
            "Number of samples: 300000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'i', 'to']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwVFbBS9AeQ1",
        "outputId": "c19c2ecb-756e-4fef-8703-99b5eda22ee1"
      },
      "source": [
        "output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  1,  1, 11,  2,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmwUG9g1Av9Y"
      },
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGoEnl9uAzed",
        "outputId": "16d1a649-6bb4-462b-94c7-aeab8ceb8299"
      },
      "source": [
        "# print(word_index)\n",
        "test = [\"the\"]\n",
        "[word_index[w] for w in test]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgzxbR65BZIq",
        "outputId": "e06435dc-ba13-41d1-f7b2-f90cf6398212"
      },
      "source": [
        "path_to_glove_file = \"/content/drive/MyDrive/glove/glove.6B.100d.txt\"\n",
        "\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfYXQcdqDmfn",
        "outputId": "734a11a2-06c1-4c14-bfbf-9eb980e3e593"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 171 words (8 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb5SnodSDsUZ"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyCEyi6YDwCM",
        "outputId": "0e44caa8-05b1-4156-bc54-53a54ff6b6f9"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_4 (Embedding)      (None, None, 100)         18100     \n",
            "_________________________________________________________________\n",
            "conv1d_21 (Conv1D)           (None, None, 128)         64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_14 (MaxPooling (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_22 (Conv1D)           (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_15 (MaxPooling (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_23 (Conv1D)           (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_7 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 263,223\n",
            "Trainable params: 245,123\n",
            "Non-trainable params: 18,100\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hVywZYYDzAM"
      },
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQz4vQjOD2eX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8550e84-324a-4886-ac5b-298374ea06b8"
      },
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 1s 942ms/step - loss: 1.1445 - acc: 0.1600 - val_loss: 1.0559 - val_acc: 0.6667\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.0454 - acc: 0.5200 - val_loss: 1.1009 - val_acc: 0.1667\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0028 - acc: 0.5200 - val_loss: 1.0519 - val_acc: 0.3333\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9688 - acc: 0.6000 - val_loss: 0.9858 - val_acc: 0.6667\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.8726 - acc: 0.6000 - val_loss: 0.9522 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7264 - acc: 0.8400 - val_loss: 0.8222 - val_acc: 0.6667\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6214 - acc: 0.8000 - val_loss: 1.0461 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6644 - acc: 0.8400 - val_loss: 0.7308 - val_acc: 0.6667\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6434 - acc: 0.6800 - val_loss: 1.1413 - val_acc: 0.3333\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5643 - acc: 0.8000 - val_loss: 0.5562 - val_acc: 0.6667\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4918 - acc: 0.7600 - val_loss: 0.9261 - val_acc: 0.6667\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.4018 - acc: 0.9600 - val_loss: 0.5893 - val_acc: 0.8333\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2771 - acc: 1.0000 - val_loss: 0.5309 - val_acc: 0.8333\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2502 - acc: 1.0000 - val_loss: 0.5152 - val_acc: 0.8333\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2069 - acc: 1.0000 - val_loss: 0.4916 - val_acc: 0.8333\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1909 - acc: 1.0000 - val_loss: 0.4066 - val_acc: 0.8333\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1591 - acc: 1.0000 - val_loss: 0.5190 - val_acc: 0.8333\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1448 - acc: 1.0000 - val_loss: 0.3182 - val_acc: 0.8333\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1514 - acc: 1.0000 - val_loss: 0.8324 - val_acc: 0.6667\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1342 - acc: 0.9600 - val_loss: 0.3314 - val_acc: 0.6667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f81d7069748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKQNw4sRD7pD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "dadaff10-3d40-4548-ceec-5c33a31965c8"
      },
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = model(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = end_to_end_model.predict(\n",
        "    # [[\"The beads are pretty but the box was broken and all the different sizes were mixed up when I got it. The box did not stand up to the shipping.\"]]\n",
        "    [[\"perfect\"]]\n",
        ")\n",
        "print(probabilities[0])\n",
        "print(np.argmax(probabilities[0]))\n",
        "class_names[np.argmax(probabilities[0])]\n",
        "# class_names[np.argmax(probabilities[0])]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f81d4a540d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[0.30988717 0.32554075 0.36457208]\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'positive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOQ3QC04_unB",
        "outputId": "e6d999fa-2fdc-43d5-ed6e-f8184bd68e56"
      },
      "source": [
        "print(vectorizer([[\"this\"]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 200), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cy5C2GQur--"
      },
      "source": [
        "\n",
        "import pickle\n",
        "pickle.dump({'config': vectorizer.get_config(),\n",
        "             'weights': vectorizer.get_weights()}\n",
        "            , open('/content/drive/MyDrive/vectorizer.pkl', \"wb\"))\n",
        "\n",
        "model.save('/content/drive/MyDrive/my_model')\n",
        "\n",
        "# vec_model = keras.models.Sequential()\n",
        "# vec_model.add(keras.Input(shape=(1,), dtype=\"string\"))\n",
        "# vec_model.add(vectorizer)\n",
        "# vec_model.save('/content/drive/MyDrive/vec_model', save_format=\"tf\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}